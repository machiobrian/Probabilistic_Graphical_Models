{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libs\n",
    "import pandas\n",
    "import numpy\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import h5py\n",
    "import types \n",
    "\n",
    "import librosa # we will make use of `features` and `filters` modules\n",
    "import hmmlearn # make use of hmm\n",
    "\n",
    "import sklearn.linear_model\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "import sklearn.metrics\n",
    "\n",
    "import functools\n",
    "import itertools\n",
    "import multiprocessing\n",
    "\n",
    "import dask_jobqueue\n",
    "import dask.distributed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpm_lower = 715\n",
    "rpm_uppper = 815\n",
    "\n",
    "win_len_ms = 1000 # sample length (ms)\n",
    "\n",
    "skip = 5000 # the number of measurements to skip from the start of each recording\n",
    "smapling_freq = 4096 # sampling rate (Hz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the recordings to be used during training\n",
    "# w_unb - without unbalance, unb - with unbalance\n",
    "wo_unb,unb = '0D', '3D'\n",
    "# define the file path of the datasets\n",
    "infile = '/home/ix502iv/Documents/Probabilistic_Graphical_Models/DataSet/dataset_hmm.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_zipfile(zfile, n):\n",
    "    win_len = int(win_len_ms/1000*smapling_freq)\n",
    "    with zfile.open(n + '.csv', 'r') as f:\n",
    "        data = pandas.read_csv(f).iloc[skip:, :]\n",
    "\n",
    "    n = (data.shape[0]//win_len) * win_len\n",
    "    data = data.iloc[:n, :]\n",
    "\n",
    "    rpm = numpy.reshape(data['Measured_RPM'].values, (-1, win_len), order='C')\n",
    "    vibr = numpy.reshape(data['Vibration_3'].values, (-1, win_len), order='C')\n",
    "    #choosing rpm based on the sensitivity aspect of hmm_mfcc : rpm_lw < rpm < rpm_up.\n",
    "    ind, = numpy.nonzero(numpy.all(rpm>rpm_lower, axis=1) & numpy.all(rpm<rpm_uppper, axis=1))\n",
    "    # randomly permutate a sequence : return a permutated range\n",
    "    numpy.random.seed(170287); ind = numpy.random.permutation(ind)\n",
    "    return vibr[ind, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, n_good, n_bad):\n",
    "    with zipfile.ZipFile(filename, 'r') as zfile:\n",
    "        good = load_zipfile(zfile, wo_unb) # 0D\n",
    "        bad = load_zipfile(zfile, unb) # 3D\n",
    "    return good, bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets load the files\n",
    "wo_unb, unb = load_data(infile, wo_unb, unb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Unb. Samples '#0D' 325\n"
     ]
    }
   ],
   "source": [
    "print(\"Without Unb. Samples '#0D'\", wo_unb.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unb. Samples '3D' 331\n"
     ]
    }
   ],
   "source": [
    "print(\"Unb. Samples '3D'\", unb.shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9 args within the train function\n",
    "def train(\n",
    "    smapling_freq,\n",
    "    wo_unb, # load the data without unbalance\n",
    "    unb, # data with unbalance\n",
    "    *,\n",
    "\n",
    "    train_ratio = 0.5, # ratio of data used for training the HMM\n",
    "\n",
    "    # Hyperparams.\n",
    "    fft_win = 31.25, # length of one fft window in milliseconds\n",
    "    hop_len = 8.0, # displacement of consecutive windows (ms)\n",
    "    n_mels = 15, # number of mel filters\n",
    "    hmm_states = 5, # number of states in the HMM\n",
    "):\n",
    "    wo_unb = int(wo_unb/1000 * smapling_freq)\n",
    "    hop_len = int(hop_len/1000 * smapling_freq)\n",
    "    mfcc_args = dict(\n",
    "        sr=smapling_freq,\n",
    "        n_fft = nfft,\n",
    "        n_mels = n_mels,\n",
    "        hop_length = hop_len\n",
    "    )\n",
    "\n",
    "    # extract the features : without the unbalance\n",
    "    tmp = [librosa.feature.mfcc(wo_unb[i,:], **mfcc_args).T\n",
    "            for i in range(wo_unb)]\n",
    "    feat_train = numpy.concatenate(tmp, axis=0)\n",
    "    train_len = [m.shape[0] for m in tmp]\n",
    "\n",
    "    # scale the features\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    scaler.fit(feat_train)\n",
    "\n",
    "    # train the HMM, on the without unb data\n",
    "    model = hmmlearn.hmm.GaussianHMM(n_components=hmm_states)\n",
    "    model.fit(scaler.transform(feat_train), length=train_len)\n",
    "\n",
    "    # compute hmm output/score for the training data : without_unbal/with_unb.\n",
    "    tmp1 = [\n",
    "        model.score(scaler.transform(\n",
    "            librosa.feature.mfcc(wo_unb[i, :], **mfcc_args).T))\n",
    "        for i in range(wo_unb, wo_unb.shape[0])]\n",
    "    tmp2 = [\n",
    "        model.score(scaler.transform(\n",
    "            librosa.feature.mfcc(unb[i,:], **mfcc_args).T))\n",
    "        for i in range(unb, unb.shape[0])]\n",
    "\n",
    "    # LogisticRegression\n",
    "    log_reg = sklearn.pipeline.make_pipeline(\n",
    "        sklearn.preprocessing.StandardScaler(),\n",
    "        sklearn.linear_model.LogisticRegression(),\n",
    "    )\n",
    "\n",
    "    log_reg.fit(\n",
    "        numpy.concatenate([numpy.reshape(tmp1, (-1,1)), numpy.reshape(tmp2, (-1,1))], axis=0),\n",
    "        numpy.array([0]*len(tmp1)+[1]*len[tmp2])\n",
    "    )\n",
    "    return mfcc_args, model, scaler, log_reg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(models, wo_unb, unb):\n",
    "    mfcc_args, model, scaler, lr = models\n",
    "\n",
    "    tmp1 = [model.score(scaler.transform(\n",
    "                librosa.feature.mfcc(wo_unb[i, :], **mfcc_args).T))\n",
    "            for i in range(wo_unb.shape[0])]\n",
    "    tmp2 = [model.score(scaler.transform(\n",
    "                librosa.feature.mfcc(unb[i,:], **mfcc_args).T))\n",
    "            for i in range(bad.shape[0])]\n",
    "            \n",
    "    result = log_reg.predict(\n",
    "        numpy.concatenate([\n",
    "            np.reshape(tmp1, (-1,1)), \n",
    "            np.reshape(tmp2, (-1,1))], axis=0))\n",
    "    actual = numpy.array([0]*len(tmp1)+[1]*len(tmp2))\n",
    "\n",
    "    return sklearn.metrics.balanced_accuracy_score(actual, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(\n",
    "    smapling_freq, wo_unb, unb, *,\n",
    "    train_samples = 200, # the number of samples for training\n",
    "    **kwargs\n",
    "):\n",
    "    m = train(smapling_freq, wo_unb[:train_samples,:], unb[:train_samples,:], **kwargs)\n",
    "    return test(m,wo_unb[train_samples:,:], unb[train_samples:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_states = [1,2,3,4,5]\n",
    "n_mels = [10,15,20]\n",
    "fft_win = [10,30,100,300]\n",
    "hop_len = [5,10,20,50]\n",
    "\n",
    "args = list(itertools.product(hmm_states, n_mels, fft_win, hop_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_score(\n\u001b[1;32m      2\u001b[0m     smapling_freq, wo_unb, unb,\n\u001b[1;32m      3\u001b[0m     hmm_states \u001b[39m=\u001b[39;49m hmm_states,\n\u001b[1;32m      4\u001b[0m     n_mels \u001b[39m=\u001b[39;49m n_mels,\n\u001b[1;32m      5\u001b[0m     fft_win \u001b[39m=\u001b[39;49m fft_win,\n\u001b[1;32m      6\u001b[0m     hop_len \u001b[39m=\u001b[39;49m hop_len\n\u001b[1;32m      7\u001b[0m )\n",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m, in \u001b[0;36mget_score\u001b[0;34m(smapling_freq, wo_unb, unb, train_samples, **kwargs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_score\u001b[39m(\n\u001b[1;32m      2\u001b[0m     smapling_freq, wo_unb, unb, \u001b[39m*\u001b[39m,\n\u001b[1;32m      3\u001b[0m     train_samples \u001b[39m=\u001b[39m \u001b[39m200\u001b[39m, \u001b[39m# the number of samples for training\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m      5\u001b[0m ):\n\u001b[0;32m----> 6\u001b[0m     m \u001b[39m=\u001b[39m train(smapling_freq, wo_unb[:train_samples,:], unb[:train_samples,:], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m      7\u001b[0m     \u001b[39mreturn\u001b[39;00m test(m,wo_unb[train_samples:,:], unb[train_samples:,:])\n",
      "Cell \u001b[0;32mIn[16], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(smapling_freq, wo_unb, unb, train_ratio, fft_win, hop_len, n_mels, hmm_states)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\n\u001b[1;32m      3\u001b[0m     smapling_freq,\n\u001b[1;32m      4\u001b[0m     wo_unb, \u001b[39m# load the data without unbalance\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     hmm_states \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m, \u001b[39m# number of states in the HMM\u001b[39;00m\n\u001b[1;32m     15\u001b[0m ):\n\u001b[0;32m---> 16\u001b[0m     wo_unb \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(wo_unb\u001b[39m/\u001b[39;49m\u001b[39m1000\u001b[39;49m \u001b[39m*\u001b[39;49m smapling_freq)\n\u001b[1;32m     17\u001b[0m     hop_len \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(hop_len\u001b[39m/\u001b[39m\u001b[39m1000\u001b[39m \u001b[39m*\u001b[39m smapling_freq)\n\u001b[1;32m     18\u001b[0m     mfcc_args \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m     19\u001b[0m         sr\u001b[39m=\u001b[39msmapling_freq,\n\u001b[1;32m     20\u001b[0m         n_fft \u001b[39m=\u001b[39m nfft,\n\u001b[1;32m     21\u001b[0m         n_mels \u001b[39m=\u001b[39m n_mels,\n\u001b[1;32m     22\u001b[0m         hop_length \u001b[39m=\u001b[39m hop_len\n\u001b[1;32m     23\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "get_score(\n",
    "    smapling_freq, wo_unb, unb,\n",
    "    hmm_states = hmm_states,\n",
    "    n_mels = n_mels,\n",
    "    fft_win = fft_win,\n",
    "    hop_len = hop_len\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45c48fb264bba0529b917885aa2fdf54bfc5ac58ac8ea30a57d1df6ad7c47fba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

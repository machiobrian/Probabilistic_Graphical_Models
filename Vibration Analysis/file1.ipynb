{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libs\n",
    "import pandas\n",
    "import numpy\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import h5py\n",
    "import types \n",
    "\n",
    "import librosa # we will make use of `features` and `filters` modules\n",
    "import hmmlearn # make use of hmm\n",
    "\n",
    "import sklearn.linear_model\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "import sklearn.metrics\n",
    "\n",
    "import functools\n",
    "import itertools\n",
    "import multiprocessing\n",
    "\n",
    "import dask_jobqueue\n",
    "import dask.distributed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpm_lower = 715\n",
    "rpm_uppper = 815\n",
    "\n",
    "win_len_ms = 1000 # sample length (ms)\n",
    "\n",
    "skip = 5000 # the number of measurements to skip from the start of each recording\n",
    "smapling_freq = 4096 # sampling rate (Hz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the recordings to be used during training\n",
    "# w_unb - without unbalance, unb - with unbalance\n",
    "wo_unb,unb = '0D', '3D'\n",
    "# define the file path of the datasets\n",
    "infile = '/home/ix502iv/Documents/Probabilistic_Graphical_Models/DataSet/dataset_hmm.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_zipfile(zfile, n):\n",
    "    win_len = int(win_len_ms/1000*smapling_freq)\n",
    "    with zfile.open(n + '.csv', 'r') as f:\n",
    "        data = pandas.read_csv(f).iloc[skip:, :]\n",
    "\n",
    "    n = (data.shape[0]//win_len) * win_len\n",
    "    data = data.iloc[:n, :]\n",
    "\n",
    "    rpm = numpy.reshape(data['Measured_RPM'].values, (-1, win_len), order='C')\n",
    "    vibr = numpy.reshape(data['Vibration_3'].values, (-1, win_len), order='C')\n",
    "    #choosing rpm based on the sensitivity aspect of hmm_mfcc : rpm_lw < rpm < rpm_up.\n",
    "    ind, = numpy.nonzero(numpy.all(rpm>rpm_lower, axis=1) & numpy.all(rpm<rpm_uppper, axis=1))\n",
    "    # randomly permutate a sequence : return a permutated range\n",
    "    numpy.random.seed(170287); ind = numpy.random.permutation(ind)\n",
    "    return vibr[ind, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, n_good, n_bad):\n",
    "    with zipfile.ZipFile(filename, 'r') as zfile:\n",
    "        good = load_zipfile(zfile, wo_unb) # 0D\n",
    "        bad = load_zipfile(zfile, unb) # 3D\n",
    "    return good, bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets load the files\n",
    "wo_unb, unb = load_data(infile, wo_unb, unb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Unb. Samples '#0D' 325\n"
     ]
    }
   ],
   "source": [
    "print(\"Without Unb. Samples '#0D'\", wo_unb.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unb. Samples '3D' 331\n"
     ]
    }
   ],
   "source": [
    "print(\"Unb. Samples '3D'\", unb.shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9 args within the train function\n",
    "def train(\n",
    "    smapling_freq,\n",
    "    wo_unb, # load the data without unbalance\n",
    "    unb, # data with unbalance\n",
    "    *,\n",
    "\n",
    "    train_ratio = 0.5, # ratio of data used for training the HMM\n",
    "\n",
    "    # Hyperparams.\n",
    "    fft_win = 31.25, # length of one fft window in milliseconds\n",
    "    hop_len = 8.0, # displacement of consecutive windows (ms)\n",
    "    n_mels = 15, # number of mel filters\n",
    "    hmm_states = 5, # number of states in the HMM\n",
    "):\n",
    "    wo_unb = int(won_trainwin/1000 * smapling_freq)\n",
    "    hop_len = int(hop_len/1000 * smapling_freq)\n",
    "    mfcc_args = dict(\n",
    "        sr=smapling_freq,\n",
    "        n_fft = nfft,\n",
    "        n_mels = n_mels,\n",
    "        hop_length = hop_len\n",
    "    )\n",
    "\n",
    "    # extract the features : without the unbalance\n",
    "    tmp = [librosa.feature.mfcc(wo_unb[i,:], **mfcc_args).T\n",
    "            for i in range(wo_unb)]\n",
    "    feat_train = numpy.concatenate(tmp, axis=0)\n",
    "    train_len = [m.shape[0] for m in tmp]\n",
    "\n",
    "    # scale the features\n",
    "    scaler = sklearn.preprocessing.StandardScaler()\n",
    "    scaler.fit(feat_train)\n",
    "\n",
    "    # train the HMM, on the without unb data\n",
    "    model = hmmlearn.hmm.GaussianHMM(n_components=hmm_states)\n",
    "    model.fit(scaler.transform(feat_train), length=train_len)\n",
    "\n",
    "    # compute hmm output/score for the training data : without_unbal/with_unb.\n",
    "    tmp1 = [\n",
    "        model.score(scaler.transform(\n",
    "            librosa.feature.mfcc(wo_unb[i, :], **mfcc_args).T))\n",
    "        for i in range(wo_unb, wo_unb.shape[0])]\n",
    "    tmp2 = [\n",
    "        model.score(scaler.transform(\n",
    "            librosa.feature.mfcc(unb[i,:], **mfcc_args).T))\n",
    "        for i in range(unb, unb.shape[0])]\n",
    "\n",
    "    # LogisticRegression\n",
    "    log_reg = sklearn.pipeline.make_pipeline(\n",
    "        sklearn.preprocessing.StandardScaler(),\n",
    "        sklearn.linear_model.LogisticRegression(),\n",
    "    )\n",
    "\n",
    "    log_reg.fit(\n",
    "        numpy.concatenate([numpy.reshape(tmp1, (-1,1)), numpy.reshape(tmp2, (-1,1))], axis=0),\n",
    "        numpy.array([0]*len(tmp1)+[1]*len[tmp2])\n",
    "    )\n",
    "    return mfcc_args, model, scaler, log_reg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(models, wo_unb, unb):\n",
    "    mfcc_args, model, scaler, lr = models\n",
    "\n",
    "    tmp1 = [model.score(scaler.transform(\n",
    "                librosa.feature.mfcc(wo_unb[i, :], **mfcc_args).T))\n",
    "            for i in range(wo_unb.shape[0])]\n",
    "    tmp2 = [model.score(scaler.transform(\n",
    "                librosa.feature.mfcc(unb[i,:], **mfcc_args).T))\n",
    "            for i in range(bad.shape[0])]\n",
    "            \n",
    "    result = log_reg.predict(\n",
    "        numpy.concatenate([\n",
    "            np.reshape(tmp1, (-1,1)), \n",
    "            np.reshape(tmp2, (-1,1))], axis=0))\n",
    "    actual = numpy.array([0]*len(tmp1)+[1]*len(tmp2))\n",
    "\n",
    "    return sklearn.metrics.balanced_accuracy_score(actual, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(\n",
    "    smapling_freq, wo_unb, unb, *,\n",
    "    train_samples = 200, # the number of samples for training\n",
    "    **kwargs\n",
    "):\n",
    "    m = train(smapling_freq, wo_unb[:train_samples,:], unb[:train_samples,:], **kwargs)\n",
    "    return test(m,wo_unb[train_samples:,:], unb[train_samples:,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
